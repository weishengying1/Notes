
服务启动脚本：
```bash
python -m vllm.entrypoints.openai.api_server  \
        --model /mnt/shared/maas/ai_story/llama3_as_en_12b_mistral_v2_1012 \
        --max-model-len 4096 \
        --gpu-memory-utilization 0.9 \
        --enable-chunked-prefill \
        --max-num-batched-tokens 512 \
        --max-num-seqs 16 \
        --enable-prefix-caching \
        --kv-cache-dtype auto \
        --dtype auto \
        --tensor-parallel-size 2 \
        --disable-log-requests
```

```bash
python -u -m vllm.entrypoints.openai.api_server --served_model_name llama2_as_def_en_12b_sfw_1115_w_1126   \
        --model /mnt/shared/maas/ai_story/llama2_as_def_en_12b_mistral_sfw_1115-W8A8-Dynamic-Per-Token \
        --gpu-memory-utilization 0.9   --max-model-len 8192  --tensor-parallel-size 1 --pipeline-parallel-size 1 \
        --enable-chunked-prefill --max-num-batched-tokens 512 --max-num-seqs 16 --enable-prefix-caching --kv-cache-dtype auto \
        --dtype auto --disable-log-requests
```
-u 是一个命令行选项，用于强制标准输入（stdin）、标准输出（stdout）和标准错误（stderr）以未缓冲模式运行(实时显示)

请求压测脚本：
```bash
python benchmarks/benchmark_serving.py \
        --backend vllm \
        --model /mnt/shared/maas/ai_story/llama3_as_en_8b_v5v1_1108 \
        --trust_remote_code \
        --dataset-name custom \
        --request-rate 4
```

关于压测的一些参数：
--request-rate ： 即QPS请求率

vllm benchmark 中，通过控制发送请求之间的时间间隔就可以实现QPS的压测，具体实现，通过伽马分布采样时间间隔，
```python
theta = 1.0 / (request_rate * burstiness)
for request in input_requests:
        yield request

        if request_rate == float("inf"):
                # If the request rate is infinity, then we don't need to wait.
                continue

        # Sample the request interval from the gamma distribution.
        # If burstiness is 1, it follows exponential distribution.
        interval = np.random.gamma(shape=burstiness, scale=theta)
        # The next request will be sent after the interval.
        await asyncio.sleep(interval)
```
np.random.gamma 就是伽马分布，其数学期望为： burstiness * theta = 1 / request_rate


--num-prompts : 即请求的次数

还有一些未使用的参数：
max_concurrency： 最大并发量，即限制推理引擎同时处理多少个请求，max_concurrency = 10， 此时已经发送了10个请求，且还没有请求有返回，则不再发送新的请求，等有回复后，再发送新的请求，直到再次达到max_concurrency。
